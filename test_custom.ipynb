{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shich\\AppData\\Roaming\\Python\\Python37\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from replknet import *\n",
    "from convert_opt_conv import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_lowest(module):\n",
    "    try:\n",
    "        first = next(module.children())\n",
    "    except:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "### dynamically decide the better operator for convolution, given parameters\n",
    "def dynamic_fit_conv(kernel_size, input_width, input_height, input_channel=3, output_channel=2):\n",
    "    # create simulated input data with batch size of 3\n",
    "    signal = torch.randn(3, input_channel, input_width, input_height)\n",
    "    kernel = torch.randn(output_channel, input_channel, kernel_size, kernel_size)\n",
    "    bias = torch.randn(output_channel)\n",
    "    \n",
    "    # initialize candidates: Conv2d, FFTConv2d\n",
    "    test_torch_conv = torch.nn.Conv2d(input_channel, output_channel, kernel_size, bias=True)\n",
    "    test_torch_conv.weight = torch.nn.Parameter(kernel)\n",
    "    test_torch_conv.bias = torch.nn.Parameter(bias)\n",
    "    \n",
    "    test_fft_conv = FFTConv2d(input_channel, output_channel, kernel_size, bias=True)\n",
    "    test_fft_conv.weight = torch.nn.Parameter(kernel)\n",
    "    test_fft_conv.bias = torch.nn.Parameter(bias)\n",
    "    \n",
    "    # run and get average time\n",
    "    iters = 10\n",
    "    time0 = time.time()\n",
    "    for i in range(iters):\n",
    "        out = test_torch_conv(signal)\n",
    "        if i == 0:\n",
    "            time0 = time.time()\n",
    "    time1 = time.time()\n",
    "    \n",
    "    for i in range(iters):\n",
    "        out = test_fft_conv(signal)\n",
    "        if i == 0:\n",
    "            time2 = time.time()\n",
    "    time3 = time.time()\n",
    "    \n",
    "    torch_time = (time1 - time0) / iters * 1000\n",
    "    fft_time = (time3 - time2) / iters * 1000\n",
    "    \n",
    "    # 0: torch conv is better; 1: fft conv is better\n",
    "    return 0 if torch_time < fft_time else 1\n",
    "\n",
    "def dynamic_convert_opt_conv2d(model, org_in_channels, org_in_width, org_in_height):\n",
    "    \n",
    "    cur_in_channels, cur_in_width, cur_in_height = org_in_channels, org_in_width, org_in_height\n",
    "    \n",
    "    # traverse through layers and substitute with optimal convolution candidate, w.r.t. key parameters (see function dynamic_fit_conv)\n",
    "    for i, submodule in enumerate(list(model.children())):\n",
    "        if not is_lowest(submodule):\n",
    "            # print('it is {}th father module'.format(i), type(submodule))\n",
    "            cur_in_channels, cur_in_width, cur_in_height = dynamic_convert_opt_conv2d(submodule, cur_in_channels, cur_in_width, cur_in_height)\n",
    "        else:\n",
    "            # print('it is a lowest module', type(submodule), is_lowest(submodule))\n",
    "            if isinstance(submodule, torch.nn.Conv2d):\n",
    "                ### fetch internal data sizes\n",
    "                ### we must feed correct kernel_size, input_size to the 'dynamic_fit_conv' function\n",
    "                # print(submodule.kernel_size[0], input_width, input_height, submodule.in_channels, submodule.out_channels)\n",
    "                best_candidate = dynamic_fit_conv(kernel_size=submodule.kernel_size[0], \n",
    "                                            input_width=cur_in_width, input_height=cur_in_height, \n",
    "                                            input_channel=submodule.in_channels, output_channel=submodule.out_channels)\n",
    "                if best_candidate == 1:\n",
    "                    # print('Convert {}th layer to FFTConv2d'.format(i + 1))\n",
    "                    # using model[i] in replacement only works with torch implemented container, like nn.Sequential, nn.ModuleList\n",
    "                    model[i] = FFTConv2d(in_channels=submodule.in_channels, \n",
    "                                                out_channels=submodule.out_channels,\n",
    "                                                kernel_size=submodule.kernel_size, \n",
    "                                                stride=submodule.stride,\n",
    "                                                padding=submodule.padding,\n",
    "                                                dilation=submodule.dilation,\n",
    "                                                groups=submodule.groups,\n",
    "                                                bias=True if submodule.bias is not None else False)\n",
    "            tmp = torch.randn(3, cur_in_channels, cur_in_width, cur_in_height)\n",
    "            cur_in_channels, cur_in_width, cur_in_height = submodule(tmp).size()[-3:]\n",
    "    return cur_in_channels, cur_in_width, cur_in_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Conv2d(3, 2, kernel_size=(30, 30), stride=(1, 1))\n",
       "    (2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Conv2d(2, 5, kernel_size=(10, 10), stride=(1, 1))\n",
       "    (1): Conv2d(5, 1, kernel_size=(30, 30), stride=(1, 1))\n",
       "    (2): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): Conv2d(1, 20, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): Conv2d(20, 3, kernel_size=(40, 40), stride=(1, 1))\n",
       "    (2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (3): Sequential(\n",
       "    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): Conv2d(3, 2, kernel_size=(60, 60), stride=(1, 1))\n",
       "    (2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define customed model 1\n",
    "module1 = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(3, 3, 3, padding=1),\n",
    "    torch.nn.Conv2d(3, 2, 30),\n",
    "    torch.nn.BatchNorm2d(2),\n",
    "    torch.nn.ReLU()\n",
    ")\n",
    "module2 = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(2, 5, 10),\n",
    "    torch.nn.Conv2d(5, 1, 30),\n",
    "    torch.nn.BatchNorm2d(1),\n",
    "    torch.nn.ReLU()\n",
    ")\n",
    "module3 = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1, 20, 3),\n",
    "    torch.nn.Conv2d(20, 3, 40),\n",
    "    torch.nn.BatchNorm2d(3),\n",
    "    torch.nn.ReLU()\n",
    ")\n",
    "module4 = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(3, 3, 3),\n",
    "    torch.nn.Conv2d(3, 2, 60),\n",
    "    torch.nn.BatchNorm2d(2),\n",
    "    torch.nn.ReLU()\n",
    ")\n",
    "myNet = torch.nn.Sequential(\n",
    "    module1, \n",
    "    module2,\n",
    "    module3,\n",
    "    module4\n",
    ")\n",
    "myNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original inference time: 9635.271310806274ms\n",
      "Convert to opt used time: 44.394498348236084s Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): _FFTConv()\n",
      "    (2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): Conv2d(2, 5, kernel_size=(10, 10), stride=(1, 1))\n",
      "    (1): _FFTConv()\n",
      "    (2): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    (0): Conv2d(1, 20, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): _FFTConv()\n",
      "    (2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (3): Sequential(\n",
      "    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): _FFTConv()\n",
      "    (2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      ")\n",
      "optimized inference time: 2257.57098197937ms\n"
     ]
    }
   ],
   "source": [
    "image = torch.randn(10, 3, 512, 512)\n",
    "time0 = time.time()\n",
    "iters = 3\n",
    "for i in range(iters):\n",
    "    out = myNet(image)\n",
    "    if i == 0:\n",
    "        time0 = time.time()\n",
    "time1 = time.time()\n",
    "org_inf_time = (time1 - time0) / (iters - 1) * 1000\n",
    "print('original inference time: {}ms'.format(org_inf_time))\n",
    "time1 = time.time()\n",
    "org_in_channels, org_in_width, org_in_height = image.size()[-3:]\n",
    "dynamic_convert_opt_conv2d(myNet, org_in_channels, org_in_width, org_in_height)\n",
    "time2 = time.time()\n",
    "convert_time = time2 - time1\n",
    "print('Convert to opt used time: {}s'.format(convert_time), myNet)\n",
    "\n",
    "time2 = time.time()\n",
    "for i in range(iters):\n",
    "    out = myNet(image)\n",
    "    if i == 0:\n",
    "        time2 = time.time()\n",
    "time3 = time.time()\n",
    "opt_inf_time = (time3 - time2) / (iters - 1) * 1000\n",
    "print('optimized inference time: {}ms'.format(opt_inf_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Conv2d(3, 2, kernel_size=(30, 30), stride=(2, 2), padding=(2, 2))\n",
       "    (2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Conv2d(2, 20, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): Conv2d(20, 30, kernel_size=(30, 30), stride=(2, 2))\n",
       "    (2): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): Conv2d(30, 3, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): Conv2d(3, 2, kernel_size=(60, 60), stride=(2, 2), padding=(2, 2))\n",
       "    (2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): AdaptiveAvgPool2d(output_size=10)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define customed model 2\n",
    "module1 = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(3, 3, 3, padding=1),\n",
    "    torch.nn.Conv2d(3, 2, 30, padding=2, stride=2),\n",
    "    torch.nn.BatchNorm2d(2),\n",
    "    torch.nn.ReLU()\n",
    ")\n",
    "module2 = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(2, 20, 3),\n",
    "    torch.nn.Conv2d(20, 30, 30, stride=2),\n",
    "    torch.nn.BatchNorm2d(30),\n",
    "    torch.nn.ReLU()\n",
    ")\n",
    "module3 = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(30, 3, 3),\n",
    "    torch.nn.Conv2d(3, 2, 60, padding=2, stride=2),\n",
    "    torch.nn.BatchNorm2d(2),\n",
    "    torch.nn.AdaptiveAvgPool2d(10)\n",
    ")\n",
    "myNet2 = torch.nn.Sequential(\n",
    "    module1, \n",
    "    module2,\n",
    "    module3\n",
    ")\n",
    "myNet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original inference time: 3422.3471879959106ms\n",
      "Convert to opt used time: 13.753021955490112s Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): _FFTConv()\n",
      "    (2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): Conv2d(2, 20, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Conv2d(20, 30, kernel_size=(30, 30), stride=(2, 2))\n",
      "    (2): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    (0): Conv2d(30, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): _FFTConv()\n",
      "    (2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): AdaptiveAvgPool2d(output_size=10)\n",
      "  )\n",
      ")\n",
      "optimized inference time: 664.1570329666138ms\n"
     ]
    }
   ],
   "source": [
    "image = torch.randn(10, 3, 512, 512)\n",
    "time0 = time.time()\n",
    "iters = 3\n",
    "for i in range(iters):\n",
    "    out = myNet2(image)\n",
    "    if i == 0:\n",
    "        time0 = time.time()\n",
    "time1 = time.time()\n",
    "org_inf_time = (time1 - time0) / (iters - 1) * 1000\n",
    "print('original inference time: {}ms'.format(org_inf_time))\n",
    "time1 = time.time()\n",
    "org_in_channels, org_in_width, org_in_height = image.size()[-3:]\n",
    "dynamic_convert_opt_conv2d(myNet2, org_in_channels, org_in_width, org_in_height)\n",
    "time2 = time.time()\n",
    "convert_time = time2 - time1\n",
    "print('Convert to opt used time: {}s'.format(convert_time), myNet2)\n",
    "\n",
    "time2 = time.time()\n",
    "for i in range(iters):\n",
    "    out = myNet2(image)\n",
    "    if i == 0:\n",
    "        time2 = time.time()\n",
    "time3 = time.time()\n",
    "opt_inf_time = (time3 - time2) / (iters - 1) * 1000\n",
    "print('optimized inference time: {}ms'.format(opt_inf_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "653e0ee8a449fd9cf9c18936d17ab66a2a2c46d6b8a7c57d62a20fb8f1602986"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
