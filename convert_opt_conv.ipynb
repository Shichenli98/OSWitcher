{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fft_conv_pytorch import fft_conv, FFTConv1d, FFTConv2d\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNet = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(3,2,5),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(2,1,3),\n",
    "    torch.nn.ReLU()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 2, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (3): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(myNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Sequential                               [1, 44, 44]               --\n",
      "├─Conv2d: 1-1                            [2, 46, 46]               152\n",
      "├─ReLU: 1-2                              [2, 46, 46]               --\n",
      "├─Conv2d: 1-3                            [1, 44, 44]               19\n",
      "├─ReLU: 1-4                              [1, 44, 44]               --\n",
      "==========================================================================================\n",
      "Total params: 171\n",
      "Trainable params: 171\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.01\n",
      "==========================================================================================\n",
      "Input size (MB): 0.03\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.08\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "mySummary = summary(myNet, (3, 50, 50))\n",
    "print(mySummary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 2, kernel_size=(5, 5), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "conv = myNet[0]\n",
    "print(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[44, 44]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mySummary.summary_list[0].output_size[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "convert_sync_batchnorm() missing 1 required positional argument: 'module'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22556/1478604521.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSyncBatchNorm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_sync_batchnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: convert_sync_batchnorm() missing 1 required positional argument: 'module'"
     ]
    }
   ],
   "source": [
    "torch.nn.SyncBatchNorm.convert_sync_batchnorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_convert_opt_conv2d(module, threshold):\n",
    "    module_output = module\n",
    "    for i, layer in enumerate(list(myNet.children())):\n",
    "        if isinstance(layer, torch.nn.Conv2d):\n",
    "            if layer.kernel_size[0] > threshold:\n",
    "                module_output[i] = FFTConv2d(in_channels=layer.in_channels, \n",
    "                                             out_channels=layer.out_channels,\n",
    "                                             kernel_size=layer.kernel_size, \n",
    "                                             stride=layer.stride,\n",
    "                                             padding=layer.padding,\n",
    "                                             padding_mode=layer.padding_mode,\n",
    "                                             dilation=layer.dilation,\n",
    "                                             groups=layer.groups,\n",
    "                                             bias=True if layer.bias is not None else False)\n",
    "    del module\n",
    "    return module_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): _FFTConv()\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(20, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (3): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "myNet_opt = hard_convert_opt_conv2d(myNet, 4)\n",
    "print(myNet_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More candidates for convolution and matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fft_conv_pytorch import fft_conv, FFTConv1d, FFTConv2d\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dynamically decide the better operator for convolution, given parameters\n",
    "def dynamic_fit_conv(kernel_size, input_width, input_height, input_channel=3, output_channel=2):\n",
    "    signal = torch.randn(3, input_channel, input_width, input_height)\n",
    "    kernel = torch.randn(output_channel, input_channel, kernel_size, kernel_size)\n",
    "    bias = torch.randn(output_channel)\n",
    "    \n",
    "    test_torch_conv = torch.nn.Conv2d(input_channel, output_channel, kernel_size, bias=True)\n",
    "    test_torch_conv.weight = torch.nn.Parameter(kernel)\n",
    "    test_torch_conv.bias = torch.nn.Parameter(bias)\n",
    "    \n",
    "    test_fft_conv = FFTConv2d(input_channel, output_channel, kernel_size, bias=True)\n",
    "    test_fft_conv.weight = torch.nn.Parameter(kernel)\n",
    "    test_fft_conv.bias = torch.nn.Parameter(bias)\n",
    "    \n",
    "    iters = 16\n",
    "    time0 = time.time()\n",
    "    for _ in range(iters):\n",
    "        out = test_torch_conv(signal)\n",
    "    time1 = time.time()\n",
    "    \n",
    "    for _ in range(iters):\n",
    "        out = test_fft_conv(signal)\n",
    "    time2 = time.time()\n",
    "    \n",
    "    torch_time = (time1 - time0) / iters * 1000\n",
    "    fft_time = (time2 - time1) / iters * 1000\n",
    "    \n",
    "    # 0: torch conv is better; 1: fft conv is better\n",
    "    return 0 if torch_time < fft_time else 1\n",
    "\n",
    "### dynamically convert current model to optimized model, accelerating convolution\n",
    "def dynamic_convert_opt_conv2d(module, org_in_channels, org_in_width, org_in_height):\n",
    "    # compute input width, height per layer\n",
    "    ####\n",
    "    mySummary = summary(module, (org_in_channels, org_in_width, org_in_height)).summary_list\n",
    "    module_output = module\n",
    "    for i, layer in enumerate(list(module.children())):\n",
    "        if isinstance(layer, torch.nn.Conv2d):\n",
    "            ### fetch internal data sizes\n",
    "            ### we must feed correct kernel_size, input_size to the 'dynamic_fit_conv' function\n",
    "            if i == 0:\n",
    "                input_width, input_height = org_in_width, org_in_height\n",
    "            else:\n",
    "                input_width, input_height = mySummary[i-1].output_size[-2:]\n",
    "            print(layer.kernel_size[0], input_width, input_height, layer.in_channels, layer.out_channels)\n",
    "            best_candidate = dynamic_fit_conv(kernel_size=layer.kernel_size[0], \n",
    "                                         input_width=input_width, input_height=input_height, \n",
    "                                         input_channel=layer.in_channels, output_channel=layer.out_channels)\n",
    "            if best_candidate == 1:\n",
    "                # org_weight, org_bias = module_output[i].weight, module_output[i].bias\n",
    "                module_output[i] = FFTConv2d(in_channels=layer.in_channels, \n",
    "                                             out_channels=layer.out_channels,\n",
    "                                             kernel_size=layer.kernel_size, \n",
    "                                             stride=layer.stride,\n",
    "                                             padding=layer.padding,\n",
    "                                             padding_mode=layer.padding_mode,\n",
    "                                             dilation=layer.dilation,\n",
    "                                             groups=layer.groups,\n",
    "                                             bias=True if layer.bias is not None else False)\n",
    "                # copy weights and bias\n",
    "                # module_output[i].weight, module_output[i].bias = org_weight, org_bias\n",
    "    del module\n",
    "    return module_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 2, kernel_size=(30, 30), stride=(1, 1))\n",
      "  (1): Conv2d(2, 2, kernel_size=(10, 10), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "myNet = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(3, 2, 30),\n",
    "    torch.nn.Conv2d(2, 2, 10)\n",
    ")\n",
    "print(myNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 474, 474])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.randn(3, 3, 512, 512)\n",
    "out = myNet(image)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 512 512 3 2\n",
      "10 474 474 2 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): _FFTConv()\n",
       "  (1): Conv2d(2, 2, kernel_size=(10, 10), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_myNet = dynamic_convert_opt_conv2d(myNet, image.shape[1], image.shape[2], image.shape[3])\n",
    "opt_myNet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
